"""
Generative Latent ODE for Household Movement Prediction.

This model learns to generate a full day's trajectory for a single person
from a single latent vector, z(0), which is derived from their static attributes.
The trajectory is generated by a Neural ODE that learns the continuous-time
dynamics of an agent's behavior.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torchdiffeq import odeint
import networkx as nx
import numpy as np

# Local imports
from ananke_abm.data_generator.mock_2p import create_two_person_training_data
from ananke_abm.data_generator.mock_locations import create_mock_zone_graph

# --- 1. Training Configuration ---

class GenerativeODEConfig:
    latent_dim: int = 32
    zone_embed_dim: int = 16
    encoder_hidden_dim: int = 128
    ode_hidden_dim: int = 128
    
    # Training parameters
    learning_rate: float = 1e-3
    num_iterations: int = 10000
    kl_weight: float = 0.01  # Weight for the KL divergence term
    
    # ODE solver settings
    ode_method: str = 'dopri5'
    
    # Purpose configuration
    purpose_groups: list = ["Home", "Work/Education", "Subsistence", "Leisure & Recreation", "Social", "Travel/Transit"]

# --- 2. Data Processor ---

class DataProcessor:
    """Processes mock data, preparing it for the Generative ODE model."""

    def __init__(self, device, config):
        self.device = device
        self.config = config
        self.sarah_data, self.marcus_data = create_two_person_training_data(repeat_pattern=False)
        self.zone_graph, _ = create_mock_zone_graph()

        # Define the mapping from detailed activities to broader categories
        self.activity_to_group = {
            # Home activities
            "sleep": "Home", "morning_routine": "Home", "evening": "Home", 
            "dinner": "Home", "arrive_home": "Home",
            # Work/Education
            "work": "Work/Education", "arrive_work": "Work/Education", "end_work": "Work/Education",
            # Subsistence
            "lunch": "Subsistence", "lunch_start": "Subsistence", "lunch_end": "Subsistence",
            # Leisure & Recreation
            "gym": "Leisure & Recreation", "gym_end": "Leisure & Recreation", 
            "exercise": "Leisure & Recreation", "leaving_park": "Leisure & Recreation",
            # Social
            "social": "Social", "leaving_social": "Social", "dinner_social": "Social",
            # Travel/Transit
            "prepare_commute": "Travel/Transit", "start_commute": "Travel/Transit",
            "transit": "Travel/Transit", "leaving_home": "Travel/Transit",
            "break": "Travel/Transit",
        }
        self.purpose_map = {name: i for i, name in enumerate(self.config.purpose_groups)}

    def get_data(self, person_id: int):
        data = self.sarah_data if person_id == 1 else self.marcus_data

        # --- Process Purpose Features ---
        activities = data["activities"]
        purpose_counts = torch.zeros(len(self.config.purpose_groups))
        for activity in activities:
            group = self.activity_to_group.get(activity, "Travel/Transit")
            group_idx = self.purpose_map[group]
            purpose_counts[group_idx] += 1
        purpose_features = F.normalize(purpose_counts, p=1, dim=0)

        return {
            "person_features": data["person_attrs"].to(self.device),
            "trajectory_y": data["zone_observations"].to(self.device),
            "times": data["times"].to(self.device),
            "num_zones": data["num_zones"],
            "home_zone_id": data["home_zone_id"],
            "work_zone_id": data["work_zone_id"],
            "person_name": data.get("person_name", f"Person {person_id}"),
            "purpose_features": purpose_features.to(self.device),
        }

# --- 3. Model Architecture ---

class ResidualBlock(nn.Module):
    """A residual block with two linear layers and a skip connection."""
    def __init__(self, dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(dim, dim), nn.Tanh(), nn.Linear(dim, dim))
        self.activation = nn.Tanh()

    def forward(self, x):
        return self.activation(x + self.net(x))

class ODEFunc(nn.Module):
    """The dynamics function f(z, t, ...) using a ResNet-like architecture."""
    def __init__(self, latent_dim, hidden_dim, static_dim, home_embed_dim):
        super().__init__()
        self.input_layer = nn.Sequential(nn.Linear(latent_dim + static_dim + home_embed_dim + 1, hidden_dim), nn.Tanh())
        self.residual_blocks = nn.Sequential(ResidualBlock(hidden_dim), ResidualBlock(hidden_dim))
        self.output_layer = nn.Linear(hidden_dim, latent_dim)

    def forward(self, t, z):
        t_vec = torch.ones(z.shape[0], 1).to(z.device) * t
        z_t_static_home = torch.cat([z, t_vec, self.static_features, self.home_zone_embedding], dim=-1)
        h = self.input_layer(z_t_static_home)
        h = self.residual_blocks(h)
        return self.output_layer(h)

class GenerativeODE(nn.Module):
    """A generative VAE model that decodes a latent vector z into a full trajectory."""
    def __init__(self, person_feat_dim, num_zones, config):
        super().__init__()
        self.config = config
        self.zone_embedder = nn.Embedding(num_zones, config.zone_embed_dim)
        
        encoder_input_dim = person_feat_dim + config.zone_embed_dim * 2 + len(config.purpose_groups)
        self.encoder = nn.Sequential(
            nn.Linear(encoder_input_dim, config.encoder_hidden_dim),
            nn.ReLU(),
            nn.Linear(config.encoder_hidden_dim, config.latent_dim * 2),
        )
        
        self.decoder = nn.Linear(config.latent_dim, num_zones)
        self.ode_func = ODEFunc(
            latent_dim=config.latent_dim, 
            hidden_dim=config.ode_hidden_dim,
            static_dim=person_feat_dim + config.zone_embed_dim, # person_features + work_embed
            home_embed_dim=config.zone_embed_dim
        )

    def forward(self, person_features, home_zone_id, work_zone_id, purpose_features, times):
        home_embed = self.zone_embedder(home_zone_id)
        work_embed = self.zone_embedder(work_zone_id)
        
        encoder_input = torch.cat([person_features, home_embed, work_embed, purpose_features], dim=-1)
        
        latent_params = self.encoder(encoder_input)
        mu, log_var = latent_params.chunk(2, dim=-1)
        
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        z0 = mu + eps * std

        self.ode_func.static_features = torch.cat([person_features, work_embed], dim=-1).expand(z0.shape[0], -1)
        self.ode_func.home_zone_embedding = home_embed.expand(z0.shape[0], -1)
        
        pred_z = odeint(self.ode_func, z0, times, method=self.config.ode_method).permute(1, 0, 2)
        pred_y_logits = self.decoder(pred_z)
        
        return pred_y_logits, mu, log_var

# --- 4. Main Execution ---

if __name__ == "__main__":
    config = GenerativeODEConfig()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    processor = DataProcessor(device, config)
    print(f"ðŸ”¬ Using device: {device}")

    print("ðŸ“Š Processing data for Person 1 (Sarah)...")
    data = processor.get_data(person_id=1)
    
    person_features = data["person_features"].unsqueeze(0)
    trajectory_y = data["trajectory_y"]
    times = data["times"]
    home_zone_id = torch.tensor([data["home_zone_id"]], device=device)
    work_zone_id = torch.tensor([data["work_zone_id"]], device=device)
    purpose_features = data["purpose_features"].unsqueeze(0)
    
    model = GenerativeODE(
        person_feat_dim=person_features.shape[-1],
        num_zones=data["num_zones"],
        config=config,
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    
    print("ðŸš€ Starting training...")
    best_loss = float('inf')
    model_path = "generative_ode_best_model.pth"

    for i in range(config.num_iterations):
        optimizer.zero_grad()
        
        pred_y_logits, mu, log_var = model(person_features, home_zone_id, work_zone_id, purpose_features, times)
        pred_y_logits = pred_y_logits.squeeze(0)

        recon_loss = F.cross_entropy(pred_y_logits, trajectory_y)
        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        loss = recon_loss + config.kl_weight * kl_loss

        loss.backward()
        optimizer.step()

        if (i + 1) % 500 == 0:
            print(f"   Iter {i+1}, Loss: {loss.item():.4f}, Recon: {recon_loss.item():.4f}, KL: {kl_loss.item():.4f}")

        if loss.item() < best_loss:
            best_loss = loss.item()
            torch.save(model.state_dict(), model_path)
            
    print("âœ… Training complete.")

    print("ðŸ“ˆ Evaluating best model...")
    model.load_state_dict(torch.load(model_path))
    model.eval()

    with torch.no_grad():
        plot_times = torch.linspace(0, 24, 100).to(device)
        pred_y_logits, _, _ = model(person_features, home_zone_id, work_zone_id, purpose_features, plot_times)
        pred_y_logits = pred_y_logits.squeeze(0)
        pred_y = torch.argmax(pred_y_logits, dim=1)

    plt.figure(figsize=(15, 6))
    plt.plot(data["times"].cpu().numpy(), data["trajectory_y"].cpu().numpy(), 'o', label='Ground Truth Snaps', markersize=8)
    plt.plot(plot_times.cpu().numpy(), pred_y.cpu().numpy(), '-', label='Generated Trajectory')
    
    plt.xlabel("Time (hours)")
    plt.ylabel("Zone ID")
    plt.title(f"Generated vs. Ground Truth Trajectory for {data['person_name']} (with Purpose)")
    plt.yticks(np.arange(data["num_zones"]))
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.legend()
    plt.tight_layout()
    plt.savefig("generative_ode_with_purpose_trajectory.png")
    print("ðŸ“„ Plot saved to 'generative_ode_with_purpose_trajectory.png'")
    plt.show() 