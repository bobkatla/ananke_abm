"""
Generative Latent ODE for Household Movement Prediction.

This model learns to generate a full day's trajectory for a single person
from a single latent vector, z(0), which is derived from their static attributes.
The trajectory is generated by a Neural ODE that learns the continuous-time
dynamics of an agent's behavior.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from dataclasses import dataclass
import os
import warnings
from torchdiffeq import odeint

# Local imports
from ananke_abm.data_generator.mock_2p import create_two_person_training_data

warnings.filterwarnings('ignore')

# --- 1. Configuration ---
@dataclass
class GenerativeODEConfig:
    """Configuration for the Generative ODE model"""
    zone_embed_dim: int = 32
    person_embed_dim: int = 32
    latent_dim: int = 64  # The dimension of the "plan" vector z
    
    ode_hidden_dim: int = 128
    
    # Training
    learning_rate: float = 0.001
    num_epochs: int = 5000
    weight_decay: float = 1e-5
    
    # ODE specific
    ode_method: str = 'dopri5'
    ode_rtol: float = 1e-4
    ode_atol: float = 1e-4

# --- 2. Data Processor ---
class DataProcessor:
    """Processes mock data, providing the raw, irregular event data."""
    
    def process_data(self):
        """Provides the original, irregularly-spaced trajectory data."""
        print("   Loading and processing data...")
        # Note: We only need Sarah's data for this single-agent model
        sarah_data, _ = create_two_person_training_data(repeat_pattern=False)

        return {
            'person_features': sarah_data['person_attrs'],
            'trajectory_y': sarah_data['zone_observations'],
            'times': sarah_data['times'],
            'num_zones': sarah_data['num_zones'],
            'zone_features': sarah_data['zone_features'],
            'person_name': sarah_data['person_name'],
            'home_zone_id': sarah_data['home_zone_id'],
            'work_zone_id': sarah_data['work_zone_id']
        }

# --- 3. Model Architecture ---

class ODEFunc(nn.Module):
    """The dynamics function f(z, t) for the Neural ODE."""
    def __init__(self, latent_dim, hidden_dim, static_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim + static_dim + 1, hidden_dim), # z, static, t
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, latent_dim),
        )

    def forward(self, t, z):
        # The ODE solver passes t and z. We need to manually append static features.
        # This is a common pattern, but requires the static_features to be an attribute.
        t_vec = torch.ones(z.shape[0], 1).to(z.device) * t
        z_t_static = torch.cat([z, t_vec, self.static_features], dim=-1)
        return self.net(z_t_static)

class GenerativeODE(nn.Module):
    """
    A generative model that decodes a latent vector z into a full trajectory
    using a Neural ODE. This version is spatially-aware.
    """
    def __init__(self, config: GenerativeODEConfig, person_feat_dim: int, num_zones: int):
        super().__init__()
        self.config = config
        
        # Zone Look-up Table
        self.zone_embedder = nn.Embedding(num_zones, config.zone_embed_dim)
        
        # Encoder: Maps enriched static features to the initial latent state z(0)
        # Input: person_features + home_zone_embedding + work_zone_embedding
        encoder_input_dim = person_feat_dim + config.zone_embed_dim + config.zone_embed_dim
        self.encoder = nn.Sequential(
            nn.Linear(encoder_input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, config.latent_dim)
        )
        
        # ODE Dynamics - conditioned on the full static feature set
        self.ode_func = ODEFunc(config.latent_dim, config.ode_hidden_dim, encoder_input_dim)
        
        # Decoder: Maps the latent trajectory z(t) to the zone embedding space
        self.decoder = nn.Linear(config.latent_dim, config.zone_embed_dim)

    def forward(self, person_features, home_zone_id, work_zone_id, times):
        """
        Generates a full trajectory from static person features.
        """
        # 1. Create enriched static features, including home and work zone embeddings
        home_zone_embed = self.zone_embedder(home_zone_id)
        work_zone_embed = self.zone_embedder(work_zone_id)
        
        enriched_static_features = torch.cat([person_features, home_zone_embed, work_zone_embed], dim=-1)

        # 2. Encode to get the initial state z(0)
        z0 = self.encoder(enriched_static_features)
        
        # 3. Pass enriched static features to the ODE for conditioning
        self.ode_func.static_features = enriched_static_features.expand(z0.shape[0], -1)

        # 4. Solve the ODE for the entire time interval
        z_t = odeint(
            self.ode_func,
            z0,
            times,
            method=self.config.ode_method,
            rtol=self.config.ode_rtol,
            atol=self.config.ode_atol
        )
        
        # 5. Decode the latent trajectory to get zone predictions
        # z_t has shape (time, batch, features), need to swap for decoder
        z_t = z_t.permute(1, 0, 2)
        pred_y_embeds = self.decoder(z_t)
        
        return pred_y_embeds

# --- 4. Training Loop ---
def train():
    """Trains the Generative ODE model."""
    print("üè† Training Generative ODE Model for a Single Agent")
    print("=" * 60)
    
    config = GenerativeODEConfig()
    processor = DataProcessor()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üî¨ Using device: {device}")

    print("üìä Processing data...")
    data = processor.process_data()
    
    model = GenerativeODE(
        config,
        person_feat_dim=data['person_features'].shape[0],
        num_zones=data['num_zones']
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    
    # Move data to device
    person_features = data['person_features'].unsqueeze(0).to(device) # Add batch dim
    home_zone_id = torch.tensor([data['home_zone_id']], device=device)
    work_zone_id = torch.tensor([data['work_zone_id']], device=device)
    trajectory_y = data['trajectory_y'].to(device)
    times = data['times'].to(device)
    
    print(f"\nüöÄ Training for {config.num_epochs} epochs...")
    save_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..', '..', 'saved_models', 'generative_ode')
    os.makedirs(save_dir, exist_ok=True)
    
    losses = []
    
    for epoch in range(config.num_epochs):
        model.train()
        optimizer.zero_grad()
        
        # Forward pass to get predicted embeddings
        pred_y_embeds = model(person_features, home_zone_id, work_zone_id, times)
        
        # Get the ground truth embeddings from the look-up table
        true_y_embeds = model.zone_embedder(trajectory_y)
        
        # Loss calculation: Use Mean Squared Error in the embedding space
        loss = F.mse_loss(pred_y_embeds.squeeze(0), true_y_embeds)
        
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        if (epoch + 1) % 100 == 0:
            print(f"   Epoch {epoch+1:5d}/{config.num_epochs} | Loss: {loss.item():.4f}")
            
    print("\n‚úÖ Training complete!")

    # --- 5. Evaluation and Visualization ---
    print("üìà Evaluating model and plotting results...")
    model.eval()
    with torch.no_grad():
        # To generate a smooth plot, we now query the ODE on a uniform grid
        plot_times = torch.linspace(0, 24, 100).to(device)
        pred_y_embeds_plot = model(person_features, home_zone_id, work_zone_id, plot_times).squeeze(0)
        
        # Find nearest zones for plotting by comparing distances in the embedding space
        all_zone_embeds = model.zone_embedder.weight
        dist_matrix = torch.cdist(pred_y_embeds_plot, all_zone_embeds)
        pred_y_plot = torch.argmin(dist_matrix, dim=1)

    plt.figure(figsize=(15, 5))
    # Plot the original, sparse ground truth points
    plt.plot(data['times'].cpu(), data['trajectory_y'].cpu(), label='Ground Truth Events', color='blue', marker='o', linestyle='None', markersize=6)
    # Plot the dense, generated trajectory from the ODE
    plt.plot(plot_times.cpu(), pred_y_plot.cpu(), label='Generated Trajectory (ODE)', color='red', linestyle='-', markersize=2)
    plt.title(f"Generated vs. Ground Truth Trajectory for {data['person_name']}")
    plt.ylabel('Zone ID')
    plt.xlabel('Time (Hours)')
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plot_path = os.path.join(save_dir, 'trajectory_comparison.png')
    plt.savefig(plot_path)
    plt.close()
    print(f"   -> Plot saved to {plot_path}")

# --- Main Execution ---
if __name__ == "__main__":
    try:
        train()
    except Exception as e:
        print(f"‚ùå Error occurred: {e}")
        import traceback
        traceback.print_exc() 